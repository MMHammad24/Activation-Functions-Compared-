The comparative evaluation of 12 state-of-the-art activation functions (AFs), based on rigorous statistical and experimental methodologies, provided valuable insights into their efficacy. This evaluation assists practitioners in making informed decisions when selecting and designing AFs for specific deep learning tasks, thereby improving the efficiency and accuracy of their models.
We also visualized the output landscape of a 3-layer randomly initialized NN with various AFs such as ReLU, ELU, SELU, GELU, Swish, HardSwish, Mish, SoftPlus, HardTanh, HardSigmoid, Sigmoid, and Tanh for visual clarity, as depicted in Fig. 91. Specifically, we input the 2-dimensional coordinates of each position in a grid into the network and plotted the scalar network output for each grid point. Our observations indicate that AFs significantly influence the smoothness of output landscapes.
ReLU, HardTanh, HardSigmoid, and HardSwish each uniquely affect NN performance. ReLU enhances sparsity and efficiency by producing zero output for negative inputs, but its abrupt transition at zero causes sharp regions in the output landscape (see Fig. 91), posing challenges for gradient-based optimization. HardTanh, which approximates the Tanh function, provides some sparsity with constant gradients within the range of -1 to 1, but introduces sharp transitions at -1 and 1, potentially affecting optimization. HardSigmoid, a piecewise linear approximation of the Sigmoid function, offers smoother transitions with a constant gradient within the range of -2.5 to 2.5, yet still has sharp regions at the boundaries, impacting optimization. HardSwish, a piecewise linear approximation of Swish, maintains smoothness within [-3,3], promoting better gradient flow and mitigating the vanishing gradient problem while being computationally efficient. These four functions can create non-smooth regions in the output landscape (and loss landscape), potentially leading to issues in gradient-based optimization. Conversely, networks using ELU, SELU, GELU, Swish, Mish, SoftPlus, Sigmoid, and Tanh exhibit considerably smoother output landscapes. Smoother output landscapes lead to smoother loss landscapes, which are easier to optimize and result in improved training and test accuracy.
